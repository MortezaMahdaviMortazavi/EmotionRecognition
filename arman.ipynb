{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel,AutoTokenizer,AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class config:\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_LENGTH = 64\n",
    "    NUM_EPOCHS = 30\n",
    "    B1 = 0.9\n",
    "    B2 = 0.999\n",
    "    _LAMBDA = 0.01\n",
    "    LEARNING_RATE = 2e-5\n",
    "    LOGFILE = 'logfile.txt'\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "    def __call__(self,text):\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalize(text)\n",
    "        # # Step 2: Remove English characters\n",
    "        text = self.remove_english_chars(text)\n",
    "        # # Step 3: Remove repeatable characters\n",
    "        # text = self.remove_repeatable_chars(text)\n",
    "        # # Step 4: Remove Arabic diacritics\n",
    "        text = self.remove_arabic_diacritics(text)\n",
    "        # # Step 5: Remove non-Persian characters\n",
    "        text = self.remove_non_persian_chars(text)\n",
    "        # # Step 6: Remove hashtags\n",
    "        text = self.remove_hashtags(text)\n",
    "        # # Step 7: Remove Persian numerals\n",
    "        text = self.remove_persian_numerals(text)\n",
    "        # text = self.remove_hash_symbol(self.lemmatize(text))\n",
    "        \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Step 1: Normalize\n",
    "    def normalize(self,text):\n",
    "        normalizer = Normalizer(\n",
    "            full_cleaning=True,\n",
    "        )\n",
    "        return normalizer.normalize(text)\n",
    "\n",
    "    def remove_hash_symbol(self,text):\n",
    "        # Remove hash symbol (#)\n",
    "        try:\n",
    "            cleaned_text = text.replace('#', '')\n",
    "        except:\n",
    "            cleaned_text = text\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "    # Step 2: remove any engish character\n",
    "    def remove_english_chars(self,text):\n",
    "        english_chars_pattern = re.compile(r'[a-zA-Z]')\n",
    "        cleaned_text = re.sub(english_chars_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4: remove arabic diactrics\n",
    "    def remove_arabic_diacritics(self,text):\n",
    "        \"\"\"\n",
    "            Some common Arabic diacritical marks include:\n",
    "                Fatha (ً): Represents the short vowel \"a\" or \"u\" when placed above a letter.\n",
    "                Kasra (ٍ): Represents the short vowel \"i\" when placed below a letter.\n",
    "                Damma (ٌ): Represents the short vowel \"u\" when placed above a letter.\n",
    "                Sukun (ـْ): Indicates the absence of any vowel sound.\n",
    "                Shadda (ّ): Represents consonant doubling or gemination.\n",
    "                Tanween (ًٌٍ): Represents the nunation or the \"n\" sound at the end of a word.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            The regular expression [\\u064B-\\u065F] represents a character range that covers the Unicode code points for Arabic diacritics.\n",
    "        \"\"\"\n",
    "        # مرحبا بكم <== \"مَرْحَبًا بِكُمْ\"\n",
    "        arabic_diacritics_pattern = re.compile(r'[\\u064B-\\u065F]')\n",
    "        cleaned_text = re.sub(arabic_diacritics_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 5: remove any non-persian chars\n",
    "    def remove_non_persian_chars(self,text):\n",
    "        import emoji\n",
    "        import regex\n",
    "\n",
    "        def get_emojis(text):\n",
    "            emojis = []\n",
    "            for char in text:\n",
    "                if regex.match(r'\\p{So}', char):\n",
    "                    emojis.append(char)\n",
    "            return emojis\n",
    "\n",
    "        # emojis = get_emojis(text)\n",
    "        persian_chars_pattern = re.compile(r'[^\\u0600-\\u06FF\\uFB8A\\u067E\\u0686\\u06AF\\u200C\\u200F]+')\n",
    "        cleaned_text = re.sub(persian_chars_pattern, ' ', text)\n",
    "        # cleaned_text += ' '.join(emojis)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 6: remove # sign from text while keeping the information included into hashtags\n",
    "    def remove_hashtags(self,text):\n",
    "        # Regular expression to match hashtags\n",
    "        hashtag_pattern = r'#\\w+'\n",
    "        \n",
    "        def extract_and_replace(match):\n",
    "            # Extract the text from the matched hashtag and remove the '#' sign\n",
    "            hashtag_text = match.group(0)[1:]\n",
    "            return hashtag_text\n",
    "        \n",
    "        # Use the 're.sub' function with the 'extract_and_replace' function as the replacement\n",
    "        cleaned_text = re.sub(hashtag_pattern, extract_and_replace, text)\n",
    "        \n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 7: remove persian numeric characters from text\n",
    "    def remove_persian_numerals(self,text):\n",
    "        # Define a translation table to map Persian numerals to None (remove them)\n",
    "        persian_numerals = {\n",
    "            ord('۰'): None,\n",
    "            ord('۱'): None,\n",
    "            ord('۲'): None,\n",
    "            ord('۳'): None,\n",
    "            ord('۴'): None,\n",
    "            ord('۵'): None,\n",
    "            ord('۶'): None,\n",
    "            ord('۷'): None,\n",
    "            ord('۸'): None,\n",
    "            ord('۹'): None\n",
    "        }\n",
    "        # Use str.translate() to remove Persian numerals\n",
    "        cleaned_text = text.translate(persian_numerals)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خیلی کوچیک هستن و سایزشون بدرد نمیخوره میخوام ...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم ب...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>چون همش با دوربین ثبت‌شده ایا میشه اعتراض زد و...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>این وضع ب طرز خنده داری گریه داره</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  emotion\n",
       "0  خیلی کوچیک هستن و سایزشون بدرد نمیخوره میخوام ...      sad\n",
       "1     از صدای پرنده دم دمای صبح متنفرم متنفرم متنفرم  disgust\n",
       "2  کیفیتش خیلی خوبه با شک خریدم ولی واقعا راضیم ب...      sad\n",
       "3  چون همش با دوربین ثبت‌شده ایا میشه اعتراض زد و...    other\n",
       "4                  این وضع ب طرز خنده داری گریه داره      sad"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = open('preprocess_data/train.txt',encoding='utf-8').readlines()\n",
    "test = open('preprocess_data/test.txt',encoding='utf-8').readlines()\n",
    "X = []\n",
    "y = []\n",
    "p = Preprocessing()\n",
    "label_mapping = {\n",
    "    'ANGRY':'anger',\n",
    "    'HATE':'disgust',\n",
    "    'FEAR':'fear',\n",
    "    'HAPPY':'joy',\n",
    "    'SAD':'sad',\n",
    "    'SURPRISE':'surprise',\n",
    "    'OTHER':'other'\n",
    "}\n",
    "for item in train:\n",
    "    _X,_y = item.split(\",\")\n",
    "    X.append(_X.strip())\n",
    "    y.append(label_mapping[_y.strip()])\n",
    "\n",
    "for item in test:\n",
    "    _X,_y = item.split(\",\")\n",
    "    X.append(_X.strip())\n",
    "    y.append(label_mapping[_y.strip()])\n",
    "arman = pd.DataFrame({'tweet': X, 'emotion': y})\n",
    "arman.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "label_mapping = {\n",
    "    'anger': 'ANGRY',\n",
    "    'disgust': 'HATE',\n",
    "    'fear': 'FEAR',\n",
    "    'joy': 'HAPPY',\n",
    "    'sad': 'SAD',\n",
    "    'surprise': 'SURPRISE'\n",
    "}\n",
    "anger = pd.read_csv('PersianTweets/anger.csv')\n",
    "disgust = pd.read_csv('PersianTweets/disgust.csv')\n",
    "fear = pd.read_csv('PersianTweets/fear.csv')\n",
    "sad = pd.read_csv('PersianTweets/sad.csv')\n",
    "joy = pd.read_csv('PersianTweets/joy.csv')\n",
    "surprise = pd.read_csv('PersianTweets/surprise.csv')\n",
    "dataset = pd.concat([anger,disgust,fear,sad,joy,surprise],axis=0)\n",
    "dataset = dataset[['tweet','emotion']]\n",
    "combined = pd.concat([dataset,arman])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/121137 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121137/121137 [06:00<00:00, 336.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "combined['tweet'] = combined['tweet'].progress_apply(p)\n",
    "combined.to_csv('full_preprocessed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, target_column, labels_dict):\n",
    "    emotions = df[target_column]\n",
    "    class_indices = emotions.map(labels_dict)\n",
    "    class_counts = class_indices.value_counts().sort_index()\n",
    "    total_samples = len(class_indices)\n",
    "    class_weights = torch.tensor(total_samples / (class_counts * len(class_counts)), dtype=torch.float)\n",
    "    return class_weights\n",
    "\n",
    "target_column = 'emotion'\n",
    "labels_dict = {'anger': 0, 'disgust': 1, 'fear': 2, 'sad': 3, 'joy': 4, 'surprise': 5, 'other': 6}\n",
    "class_weights = calculate_class_weights(combined, target_column, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sad         35493\n",
       "joy         28925\n",
       "anger       21149\n",
       "fear        18440\n",
       "surprise    13745\n",
       "other        1879\n",
       "disgust      1506\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X , y = dataset['tweet'].tolist() , dataset['emotion'].tolist()\n",
    "train , test = train_test_split(combined,test_size=0.1,stratify=combined['emotion'])\n",
    "X_train , y_train = train['tweet'].tolist() , train['emotion'].tolist()\n",
    "X_test , y_test = test['tweet'].tolist() , test['emotion'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "        self.max_length = config.MAX_LENGTH\n",
    "        # self.labels_dict = {'SAD': 0, 'HAPPY': 1,'SURPRISE': 2, 'FEAR': 3, 'HATE': 4, 'ANGRY': 5,'OTHER': 6}\n",
    "        self.labels_dict = {'anger':0,'disgust':1,'fear':2,'sad':3,'joy':4,'surprise':5,'other':6}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        # self.tokenizer.add_special_tokens(text)\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels_dict[label])\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifierFactory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model(model_name, num_labels):\n",
    "        model_name = model_name.lower()\n",
    "        if model_name == 'parsbert':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\", num_labels=num_labels)\n",
    "        elif model_name == 'xlm-roberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained('checkpoints/models--xlm-roberta-large/snapshots/')\n",
    "            model = AutoModelForSequenceClassification.from_pretrained('checkpoints/models--xlm-roberta-large/snapshots/', num_labels=num_labels)\n",
    "        elif model_name == 'xlm-emo':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"m3hrdadfi/xlm-mlm-17-1280-emotion\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"m3hrdadfi/xlm-mlm-17-1280-emotion\", num_labels=num_labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' not supported.\")\n",
    "        \n",
    "        return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig , AutoTokenizer , AutoModel\n",
    "\n",
    "def change_grad(module,_req_grad_=False):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = _req_grad_\n",
    "    \n",
    "num_labels = 7\n",
    "model_name = 'parsbert'\n",
    "tokenizer , model = SequenceClassifierFactory().create_model(model_name,num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_dataset = TextDataset(X_train,y_train,max_length=100)\n",
    "test_dataset = TextDataset(X_test,y_test,max_length=100)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch: 0 loss: 1.8143632411956787\n",
      "Epoch: 0 batch: 25 loss: 1.541748046875\n",
      "Epoch: 0 batch: 50 loss: 1.3507308959960938\n",
      "Epoch: 0 batch: 75 loss: 0.7798649668693542\n",
      "Epoch: 0 batch: 100 loss: 0.5820590257644653\n",
      "Epoch: 0 batch: 125 loss: 0.3716413080692291\n",
      "Epoch: 0 batch: 150 loss: 0.35400620102882385\n",
      "Epoch: 0 batch: 175 loss: 0.26377174258232117\n",
      "Epoch: 0 batch: 200 loss: 0.39654573798179626\n",
      "Epoch: 0 batch: 225 loss: 0.3441065549850464\n",
      "Epoch: 0 batch: 250 loss: 0.3368087410926819\n",
      "Epoch: 0 batch: 275 loss: 0.1494172215461731\n",
      "Epoch: 0 batch: 300 loss: 0.21539512276649475\n",
      "Epoch: 0 batch: 325 loss: 0.2687482535839081\n",
      "Epoch: 0 batch: 350 loss: 0.21493662893772125\n",
      "Epoch: 0 batch: 375 loss: 0.08349525928497314\n",
      "Epoch: 0 batch: 400 loss: 0.17957624793052673\n",
      "Epoch: 0 batch: 425 loss: 0.17172101140022278\n",
      "Epoch: 0 batch: 450 loss: 0.0331953689455986\n",
      "Epoch: 0 batch: 475 loss: 0.1509796530008316\n",
      "Epoch: 0 batch: 500 loss: 0.16187503933906555\n",
      "Epoch: 0 batch: 525 loss: 0.18249565362930298\n",
      "Epoch: 0 batch: 550 loss: 0.17436395585536957\n",
      "Epoch: 0 batch: 575 loss: 0.2281971126794815\n",
      "Epoch: 0 batch: 600 loss: 0.2780027389526367\n",
      "Epoch: 0 batch: 625 loss: 0.15681256353855133\n",
      "Epoch: 0 batch: 650 loss: 0.26800471544265747\n",
      "Epoch: 0 batch: 675 loss: 0.12560893595218658\n",
      "Epoch: 0 batch: 700 loss: 0.10223007202148438\n",
      "Epoch: 0 batch: 725 loss: 0.12300132215023041\n",
      "Epoch: 0 batch: 750 loss: 0.192081019282341\n",
      "Epoch: 0 batch: 775 loss: 0.353887677192688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9132\\2824637618.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Backward pass and update model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.cuda())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "model = model.cuda()\n",
    "for _iter in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    model.train()\n",
    "    for idx,batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        outputs = model(input_ids,attention_mask).logits\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        # Backward pass and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        train_predictions.extend(batch_predictions)\n",
    "        train_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        train_loss+=loss.item()\n",
    "        if idx % 50 == 0:\n",
    "            print(\"Epoch:\",_iter,\"batch:\",idx,\"loss:\",loss.item())\n",
    "\n",
    "        # Calculate accuracy and F-score\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    train_fscore = f1_score(train_labels, train_predictions, average='macro')\n",
    "\n",
    "    print(\"Total Loss:\", train_loss / len(train_dataloader))\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Train F-score:\", train_fscore)\n",
    "\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        outputs = model(input_ids, attention_mask).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        test_predictions.extend(batch_predictions)\n",
    "        test_labels.extend(labels.cpu().tolist())\n",
    "        test_loss += loss.item()\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    test_fscore = f1_score(test_labels, test_predictions, average='macro')\n",
    "    print(\"Total Loss (Test):\", test_loss / len(test_dataloader))\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Test F-score:\", test_fscore)\n",
    "\n",
    "    torch.save(model.state_dict(),'checkpoints/ParsBertOnTweets100k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "77333",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 77333",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14016\\3892551431.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14016\\92355963.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14016\\92355963.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.6\u001b[0m  \u001b[1;31m# Set the threshold value as per your requirements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;31m# for batch in tqdm(self.train_loader):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14016\\1459835806.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# self.tokenizer.add_special_tokens(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 77333"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# trainer = Trainer(model=model,train_loader=train_dataloader,val_loader=test_dataloader)\n",
    "# trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1418.10it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 84.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(texts,local_ids, model, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    inputs = []  # To store the input tensors\n",
    "    # anger, sadness, surprise, happiness, fear, disgust, other\n",
    "    label_dict = {0: 'sadness', 1: 'happiness', 2: 'surprise', 3: 'fear', 4: 'disgust', 5: 'anger', 6: 'other'}\n",
    "    num_classes = len(label_dict)\n",
    "    # texts = texts['tweet'].tolist()\n",
    "    # local_id = texts['local_id'].tolist()\n",
    "    \n",
    "    # Tokenize and preprocess the texts\n",
    "    tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')\n",
    "    max_length = 64\n",
    "    for text in tqdm(texts):\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze().to(device)\n",
    "        attention_mask = encoding['attention_mask'].squeeze().to(device)\n",
    "        inputs.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        prob_matrix = []  # To store the probability distribution for each text\n",
    "        for input_data in tqdm(inputs):\n",
    "            input_ids = input_data['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = input_data['attention_mask'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            predicted_probs = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
    "            prob_matrix.append(predicted_probs)\n",
    "            \n",
    "            # Apply threshold to determine emotions present in text\n",
    "            predicted_labels = [label_dict[idx] for idx, prob in enumerate(predicted_probs) if prob > threshold]\n",
    "            predictions.append(label_dict[torch.argmax(outputs, dim=1).item()])\n",
    "\n",
    "    csv_data = []\n",
    "    for local_id, text, primary_emotion, probs in zip(local_ids, texts, predictions, prob_matrix):\n",
    "        emotion_probs = [1 if prob > threshold else 0 for prob in probs]\n",
    "        row = [local_id, text, primary_emotion] + emotion_probs\n",
    "        csv_data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the CSV data\n",
    "    columns = [\"local_id\", \"tweet\", \"primary_emotion\", \"anger\", \"sadness\", \"fear\", \"happiness\", \"disgust\", \"surprise\", \"other\"]\n",
    "    df = pd.DataFrame(csv_data, columns=columns)\n",
    "    df.to_csv('checkpoints/predictions_2.csv', index=False)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "texts = pd.read_csv('final/data_emotion_without_label.csv',encoding='utf-8')['tweet'].tolist()\n",
    "local_id = pd.read_csv('final/data_emotion_without_label.csv',encoding='utf-8')['local_id'].tolist()\n",
    "\n",
    "x = predict(texts=texts,local_ids=local_id,model=model,device='cuda',threshold=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom imblearn.over_sampling import SMOTE\\nimport nlpaug.augmenter.word as naw\\n\\n# Load a text dataset (e.g., 20 Newsgroups)\\nnewsgroups = fetch_20newsgroups(subset=\\'all\\', remove=(\\'headers\\', \\'footers\\', \\'quotes\\'))\\n\\nX, y = newsgroups.data, newsgroups.target\\n\\n# Split the dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Apply TF-IDF vectorization\\nvectorizer = TfidfVectorizer(max_features=10000)\\nX_train_tfidf = vectorizer.fit_transform(X_train)\\n\\n# Apply oversampling using SMOTE\\noversampler = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\\n\\n# Apply data augmentation using nlpaug\\naug = naw.SynonymAug(aug_src=\\'wordnet\\', aug_max=10)\\nX_train_augmented = [aug.augment(text) for text in X_train]\\n\\n# Print sample sizes before and after resampling/augmentation\\nprint(\"Original class distribution:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\\nprint(\"After SMOTE resampling:\", {class_label: sum(y_train_resampled == class_label) for class_label in set(y_train_resampled)})\\nprint(\"After data augmentation:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\\n\\n# Now you can use X_train_resampled and y_train_resampled for training\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Oversampling and Undersampling: Oversampling and undersampling techniques can be used with text data, but there are some challenges. In oversampling, simply duplicating text samples might not be effective, as it can lead to overfitting. Techniques like SMOTE or generating synthetic examples through text augmentation (e.g., paraphrasing) can be more appropriate. For undersampling, randomly removing text samples may result in loss of valuable information. Undersampling methods that consider text properties, like Tomek links, might be more suitable.\n",
    "\n",
    "Synthetic Data Generation: Generating synthetic examples for text data can be more complex than in tabular data. Techniques like SMOTE might need adaptations to consider the sequential nature of text. Additionally, methods like Word2Vec or GPT-based language models can be used for generating semantically similar but contextually different text samples.\n",
    "\n",
    "Data Augmentation: Data augmentation methods for text involve introducing variations in text content while preserving meaning. Techniques like synonym replacement, word swapping, and paraphrasing can be used to create augmented versions of the minority class text samples.\n",
    "\n",
    "Class-Weighted Loss: Class-weighted loss can be applied to text data as well. It considers the importance of each class in the loss calculation during training.\n",
    "\n",
    "Imbalanced-learn Library: If you're using Python, the imbalanced-learn library offers resampling techniques tailored for imbalanced datasets. While some techniques might need adjustments for text data, the library can still provide a good starting point.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load a text dataset (e.g., 20 Newsgroups)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply oversampling using SMOTE\n",
    "oversampler = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Apply data augmentation using nlpaug\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_max=10)\n",
    "X_train_augmented = [aug.augment(text) for text in X_train]\n",
    "\n",
    "# Print sample sizes before and after resampling/augmentation\n",
    "print(\"Original class distribution:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\n",
    "print(\"After SMOTE resampling:\", {class_label: sum(y_train_resampled == class_label) for class_label in set(y_train_resampled)})\n",
    "print(\"After data augmentation:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\n",
    "\n",
    "# Now you can use X_train_resampled and y_train_resampled for training\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any\n",
    "import hazm\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from dataclasses import dataclass\n",
    "from hazm import Lemmatizer\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self,text):\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_english_chars(text)\n",
    "        # text = self.remove_repeatable_chars(text)\n",
    "        text = self.remove_arabic_diacritics(text)\n",
    "        text = self.remove_non_persian_chars(text)\n",
    "\n",
    "        text = self.remove_persian_numerals(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        return text\n",
    "    \n",
    "    # Step 1: Normalize\n",
    "    def normalize(self,text):\n",
    "        normalizer = Normalizer(\n",
    "            full_cleaning=True,\n",
    "        )\n",
    "        return normalizer.normalize(text)\n",
    "\n",
    "    # Step 2: remove any engish character\n",
    "    def remove_english_chars(self,text):\n",
    "        english_chars_pattern = re.compile(r'[a-zA-Z]')\n",
    "        cleaned_text = re.sub(english_chars_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 3: remove repeatable characters\n",
    "    def remove_repeatable_chars(self,text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    # Step 4: remove arabic diactrics\n",
    "    def remove_arabic_diacritics(self,text):\n",
    "        \"\"\"\n",
    "            Some common Arabic diacritical marks include:\n",
    "                Fatha (ً): Represents the short vowel \"a\" or \"u\" when placed above a letter.\n",
    "                Kasra (ٍ): Represents the short vowel \"i\" when placed below a letter.\n",
    "                Damma (ٌ): Represents the short vowel \"u\" when placed above a letter.\n",
    "                Sukun (ـْ): Indicates the absence of any vowel sound.\n",
    "                Shadda (ّ): Represents consonant doubling or gemination.\n",
    "                Tanween (ًٌٍ): Represents the nunation or the \"n\" sound at the end of a word.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            The regular expression [\\u064B-\\u065F] represents a character range that covers the Unicode code points for Arabic diacritics.\n",
    "        \"\"\"\n",
    "        # مرحبا بكم <== \"مَرْحَبًا بِكُمْ\"\n",
    "        arabic_diacritics_pattern = re.compile(r'[\\u064B-\\u065F]')\n",
    "        cleaned_text = re.sub(arabic_diacritics_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 5: remove any non-persian chars\n",
    "    def remove_non_persian_chars(self,text):\n",
    "        persian_chars_pattern = re.compile(r'[^\\u0600-\\u06FF\\uFB8A\\u067E\\u0686\\u06AF\\u200C\\u200F]+')\n",
    "        cleaned_text = re.sub(persian_chars_pattern, ' ', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 6: remove # sign from text while keeping the information included into hashtags\n",
    "    def remove_hashtags(self,text):\n",
    "        # Regular expression to match hashtags\n",
    "        hashtag_pattern = r'#\\w+'\n",
    "        \n",
    "        def extract_and_replace(match):\n",
    "            # Extract the text from the matched hashtag and remove the '#' sign\n",
    "            hashtag_text = match.group(0)[1:]\n",
    "            return hashtag_text\n",
    "        \n",
    "        # Use the 're.sub' function with the 'extract_and_replace' function as the replacement\n",
    "        cleaned_text = re.sub(hashtag_pattern, extract_and_replace, text)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "\n",
    "    # Step 7: remove persian numeric characters from text\n",
    "    def remove_persian_numerals(self,text):\n",
    "        # Define a translation table to map Persian numerals to None (remove them)\n",
    "        persian_numerals = {\n",
    "            ord('۰'): None,\n",
    "            ord('۱'): None,\n",
    "            ord('۲'): None,\n",
    "            ord('۳'): None,\n",
    "            ord('۴'): None,\n",
    "            ord('۵'): None,\n",
    "            ord('۶'): None,\n",
    "            ord('۷'): None,\n",
    "            ord('۸'): None,\n",
    "            ord('۹'): None\n",
    "        }\n",
    "        # Use str.translate() to remove Persian numerals\n",
    "        cleaned_text = text.translate(persian_numerals)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_emoPars.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17304\\2476935492.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_emoPars.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_emoPars.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_emoPars.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "train_file = pd.read_csv('train_emoPars.csv',encoding='utf-8')\n",
    "test_file = pd.read_csv('test_emoPars.csv',encoding='utf-8')\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "train_file['text'] = train_file['text'].progress_apply(p)\n",
    "test_file['text'] = test_file['text'].progress_apply(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.to_csv('train_cleaned_emopars.csv')\n",
    "test_file.to_csv('test_cleaned_emopars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.to_csv('test_cleaned_emopars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       من خیلی خودسانسوری می‌کنم تو اینستا هر چی فالو...\n",
       "1               بعد اتمام جلسه مجلس روند بازار برگشت بورس\n",
       "2       کاربران توییتر در جریان طوفان توییتری اعتراض ب...\n",
       "3       وحشی شدن معده بعد از رسیدن به ایران اجتناب ناپ...\n",
       "4       سحام نیوز بیانیه مشترک عربستان و امارات با پرو...\n",
       "                              ...                        \n",
       "2995                    بعد از کرونا یه صفایی بکنیم با هم\n",
       "2996                              قبلش یه پیتزا بده حداقل\n",
       "2997    از رفقا می‌خواهم که از بین اپوزیسیون حزب یا سا...\n",
       "2998    از هر دریچه‌ای؛ چه موازنه قوا با امریکا چه تام...\n",
       "2999          چند نفر تا حتما هستن میخوام آمار بگیرم بگین\n",
       "Name: texts, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 219.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "y = pd.read_csv('datasets/data_emotion_without_label.csv')\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "y['tweet'] = y['tweet'].progress_apply(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv('data_emotion_without_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7308/7308 [00:18<00:00, 386.83it/s]\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train,test],axis=0)\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "combined['text'] = combined['text'].progress_apply(p)\n",
    "combined.to_csv('final/arman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('checkpoints/predictions_1.csv')\n",
    "y = pd.read_csv('res.csv')\n",
    "y['primary_emotion'] = x['primary_emotion']\n",
    "y.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = pd.read_csv('PersianTweets/anger.csv')\n",
    "disgust = pd.read_csv('PersianTweets/disgust.csv')\n",
    "fear = pd.read_csv('PersianTweets/fear.csv')\n",
    "joy = pd.read_csv('PersianTweets/joy.csv')\n",
    "sad = pd.read_csv('PersianTweets/sad.csv')\n",
    "surprise = pd.read_csv('PersianTweets/surprise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20069, 925, 17624, 28024, 34328, 12859)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anger) , len(disgust) , len(fear) , len(joy) , len(sad) , len(surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.concat([anger,disgust,fear,joy,sad,surprise],axis=0)\n",
    "all_tweets.to_csv('PersianTweets/allDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>دیشب خواب دیدم بمبی چیزی زدن نورش خیلی خیره کن...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['No2IR']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>تبر زدی بر ریشه‌اَم، جوانه رویید جایِ زخم\\nران...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>['سين_کاف']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>پدر سوخته ای که بابام بهم میگه دو معنی داره که...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>['پدر_ایرانی']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>با خود مواجه شوید و اخم نکنید. اقتدار در نگاه ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['جذبه', 'اخم']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>مدح تو را در شادی و در غم نوشتند\\nبا این همه ا...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>['نبأ_عظیم']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113824</th>\n",
       "      <td>12854</td>\n",
       "      <td>متعجبم چرا لیبرالها و غرب گداها و فتنه‌گران سا...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['دلار_اصلاح_طلبان']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113825</th>\n",
       "      <td>12855</td>\n",
       "      <td>چیزی که متعجبم میکنه اینکه از حق خودمختاری مرد...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['ترکمنچای_چینی']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113826</th>\n",
       "      <td>12856</td>\n",
       "      <td>متعجبم از اون دسته عزیزانی که هنوز هشتگ #رای_ب...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>['رای_بی_رای', 'نه_به_جمهوی_اسلامی']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113827</th>\n",
       "      <td>12857</td>\n",
       "      <td>#ظریف دهان همه منتقدانش را بست؟؟؟؟!!!!..... حق...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>['ظریف']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113828</th>\n",
       "      <td>12858</td>\n",
       "      <td>دلم برای مردمم میسوزد و متعجبم که با ذوق با #ظ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>['ظریف', 'كلابهاوس', 'احمدی_نژاد']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113829 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                              tweet  \\\n",
       "0                0  دیشب خواب دیدم بمبی چیزی زدن نورش خیلی خیره کن...   \n",
       "1                1  تبر زدی بر ریشه‌اَم، جوانه رویید جایِ زخم\\nران...   \n",
       "2                2  پدر سوخته ای که بابام بهم میگه دو معنی داره که...   \n",
       "3                3  با خود مواجه شوید و اخم نکنید. اقتدار در نگاه ...   \n",
       "4                4  مدح تو را در شادی و در غم نوشتند\\nبا این همه ا...   \n",
       "...            ...                                                ...   \n",
       "113824       12854  متعجبم چرا لیبرالها و غرب گداها و فتنه‌گران سا...   \n",
       "113825       12855  چیزی که متعجبم میکنه اینکه از حق خودمختاری مرد...   \n",
       "113826       12856  متعجبم از اون دسته عزیزانی که هنوز هشتگ #رای_ب...   \n",
       "113827       12857  #ظریف دهان همه منتقدانش را بست؟؟؟؟!!!!..... حق...   \n",
       "113828       12858  دلم برای مردمم میسوزد و متعجبم که با ذوق با #ظ...   \n",
       "\n",
       "        replyCount  retweetCount  likeCount  quoteCount  \\\n",
       "0                0             3          2           0   \n",
       "1                0             0          8           0   \n",
       "2                1             0         11           0   \n",
       "3                0             0          1           0   \n",
       "4                4             6         36           0   \n",
       "...            ...           ...        ...         ...   \n",
       "113824           0             0          2           0   \n",
       "113825           1             0          0           0   \n",
       "113826           0             0          2           0   \n",
       "113827           0             1         48           0   \n",
       "113828           0             0          5           0   \n",
       "\n",
       "                                    hashtags          sourceLabel   emotion  \n",
       "0                                  ['No2IR']      Twitter Web App     anger  \n",
       "1                                ['سين_کاف']  Twitter for Android     anger  \n",
       "2                             ['پدر_ایرانی']  Twitter for Android     anger  \n",
       "3                            ['جذبه', 'اخم']   Twitter for iPhone     anger  \n",
       "4                               ['نبأ_عظیم']      Twitter Web App     anger  \n",
       "...                                      ...                  ...       ...  \n",
       "113824                  ['دلار_اصلاح_طلبان']  Twitter for Android  surprise  \n",
       "113825                     ['ترکمنچای_چینی']   Twitter for iPhone  surprise  \n",
       "113826  ['رای_بی_رای', 'نه_به_جمهوی_اسلامی']  Twitter for Android  surprise  \n",
       "113827                              ['ظریف']  Twitter for Android  surprise  \n",
       "113828    ['ظریف', 'كلابهاوس', 'احمدی_نژاد']   Twitter for iPhone  surprise  \n",
       "\n",
       "[113829 rows x 9 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('PersianTweets/allDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
