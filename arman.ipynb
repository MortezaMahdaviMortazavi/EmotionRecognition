{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel,AutoTokenizer,AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class config:\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_LENGTH = 64\n",
    "    NUM_EPOCHS = 30\n",
    "    B1 = 0.9\n",
    "    B2 = 0.999\n",
    "    _LAMBDA = 0.01\n",
    "    LEARNING_RATE = 2e-5\n",
    "    LOGFILE = 'logfile.txt'\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "    def __call__(self,text):\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalize(text)\n",
    "        # # Step 2: Remove English characters\n",
    "        text = self.remove_english_chars(text)\n",
    "        # # Step 3: Remove repeatable characters\n",
    "        # text = self.remove_repeatable_chars(text)\n",
    "        # # Step 4: Remove Arabic diacritics\n",
    "        text = self.remove_arabic_diacritics(text)\n",
    "        # # Step 5: Remove non-Persian characters\n",
    "        text = self.remove_non_persian_chars(text)\n",
    "        # # Step 6: Remove hashtags\n",
    "        text = self.remove_hashtags(text)\n",
    "        # # Step 7: Remove Persian numerals\n",
    "        text = self.remove_persian_numerals(text)\n",
    "        # text = self.remove_hash_symbol(self.lemmatize(text))\n",
    "        \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # Step 1: Normalize\n",
    "    def normalize(self,text):\n",
    "        normalizer = Normalizer(\n",
    "            full_cleaning=True,\n",
    "        )\n",
    "        return normalizer.normalize(text)\n",
    "\n",
    "    def remove_hash_symbol(self,text):\n",
    "        # Remove hash symbol (#)\n",
    "        try:\n",
    "            cleaned_text = text.replace('#', '')\n",
    "        except:\n",
    "            cleaned_text = text\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "    # Step 2: remove any engish character\n",
    "    def remove_english_chars(self,text):\n",
    "        english_chars_pattern = re.compile(r'[a-zA-Z]')\n",
    "        cleaned_text = re.sub(english_chars_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4: remove arabic diactrics\n",
    "    def remove_arabic_diacritics(self,text):\n",
    "        \"\"\"\n",
    "            Some common Arabic diacritical marks include:\n",
    "                Fatha (ً): Represents the short vowel \"a\" or \"u\" when placed above a letter.\n",
    "                Kasra (ٍ): Represents the short vowel \"i\" when placed below a letter.\n",
    "                Damma (ٌ): Represents the short vowel \"u\" when placed above a letter.\n",
    "                Sukun (ـْ): Indicates the absence of any vowel sound.\n",
    "                Shadda (ّ): Represents consonant doubling or gemination.\n",
    "                Tanween (ًٌٍ): Represents the nunation or the \"n\" sound at the end of a word.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            The regular expression [\\u064B-\\u065F] represents a character range that covers the Unicode code points for Arabic diacritics.\n",
    "        \"\"\"\n",
    "        # مرحبا بكم <== \"مَرْحَبًا بِكُمْ\"\n",
    "        arabic_diacritics_pattern = re.compile(r'[\\u064B-\\u065F]')\n",
    "        cleaned_text = re.sub(arabic_diacritics_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 5: remove any non-persian chars\n",
    "    def remove_non_persian_chars(self,text):\n",
    "        import emoji\n",
    "        import regex\n",
    "\n",
    "        def get_emojis(text):\n",
    "            emojis = []\n",
    "            for char in text:\n",
    "                if regex.match(r'\\p{So}', char):\n",
    "                    emojis.append(char)\n",
    "            return emojis\n",
    "\n",
    "        # emojis = get_emojis(text)\n",
    "        persian_chars_pattern = re.compile(r'[^\\u0600-\\u06FF\\uFB8A\\u067E\\u0686\\u06AF\\u200C\\u200F]+')\n",
    "        cleaned_text = re.sub(persian_chars_pattern, ' ', text)\n",
    "        # cleaned_text += ' '.join(emojis)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 6: remove # sign from text while keeping the information included into hashtags\n",
    "    def remove_hashtags(self,text):\n",
    "        # Regular expression to match hashtags\n",
    "        hashtag_pattern = r'#\\w+'\n",
    "        \n",
    "        def extract_and_replace(match):\n",
    "            # Extract the text from the matched hashtag and remove the '#' sign\n",
    "            hashtag_text = match.group(0)[1:]\n",
    "            return hashtag_text\n",
    "        \n",
    "        # Use the 're.sub' function with the 'extract_and_replace' function as the replacement\n",
    "        cleaned_text = re.sub(hashtag_pattern, extract_and_replace, text)\n",
    "        \n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 7: remove persian numeric characters from text\n",
    "    def remove_persian_numerals(self,text):\n",
    "        # Define a translation table to map Persian numerals to None (remove them)\n",
    "        persian_numerals = {\n",
    "            ord('۰'): None,\n",
    "            ord('۱'): None,\n",
    "            ord('۲'): None,\n",
    "            ord('۳'): None,\n",
    "            ord('۴'): None,\n",
    "            ord('۵'): None,\n",
    "            ord('۶'): None,\n",
    "            ord('۷'): None,\n",
    "            ord('۸'): None,\n",
    "            ord('۹'): None\n",
    "        }\n",
    "        # Use str.translate() to remove Persian numerals\n",
    "        cleaned_text = text.translate(persian_numerals)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.read_csv('preprocess_data/X_train.csv',encoding='utf-8')\n",
    "# X_test = pd.read_csv('preprocess_data/X_test.csv',encoding='utf-8')\n",
    "# y_train = pd.read_csv('preprocess_data/y_train.csv')\n",
    "# y_test = pd.read_csv('preprocess_data/y_test.csv')\n",
    "# # X_train = X_train.dropna()\n",
    "# # X_test = X_test.dropna()\n",
    "# # y_train = y_train.dropna()\n",
    "# # y_test = y_test.dropna()\n",
    "\n",
    "# # X_train = X_train['text'].to_list()\n",
    "# # X_test = X_test['text'].to_list()\n",
    "\n",
    "# # y_train = y_train['label'].to_list()\n",
    "# # y_test = y_test['label'].to_list()\n",
    "# X_test[-1],y_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = open('data/Augmented_train.txt',encoding='utf-8').readlines()\n",
    "# test = open('preprocess_data/test.txt',encoding='utf-8').readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open('preprocess_data/train.txt',encoding='utf-8').readlines()\n",
    "test = open('preprocess_data/test.txt',encoding='utf-8').readlines()\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "p = Preprocessing()\n",
    "for item in train:\n",
    "    X,y = item.split(\",\")\n",
    "    X_train.append(p(X.strip()))\n",
    "    y_train.append(y.rstrip())\n",
    "\n",
    "for item in test:\n",
    "    X,y = item.split(\",\")\n",
    "    X_test.append(p(X.strip()))\n",
    "    y_test.append(y.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.read_csv('data/cleaned_emopars.csv')\n",
    "X_train.extend(cleaned['text'].tolist())\n",
    "y_train.extend(cleaned['major_emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def write_text_to_file(text,file_path):\n",
    "#     with open(file_path, 'a',encoding='utf-8') as _file:\n",
    "#         _file.write(text)\n",
    "        \n",
    "# preprocessor = Preprocessing()\n",
    "# augments = pd.read_csv('data/augmented.csv',encoding='utf-8')\n",
    "# texts = augments['text'].tolist()\n",
    "# new_texts = []\n",
    "# labels = augments['label'].tolist()\n",
    "# idx = 0\n",
    "# for text in tqdm(texts):\n",
    "#     writed = preprocessor(text) + \",\" +str(labels[idx])+\"\\n\"\n",
    "#     write_text_to_file(writed,'train.txt')\n",
    "#     idx+=1\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANGER        10530\n",
       "FEAR          5009\n",
       "SADNESS       4167\n",
       "HATRED        4060\n",
       "HAPPINESS     3753\n",
       "WONDER        2363\n",
       "Name: major_emotion, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned['major_emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "        self.max_length = config.MAX_LENGTH\n",
    "        self.labels_dict = {'SAD': 0, 'HAPPY': 1,'SURPRISE': 2, 'FEAR': 3, 'HATE': 4, 'ANGRY': 5,'OTHER': 6,'HATRED':4,'WONDER':2,'ANGER':5,'SADNESS':0,'HAPPINESS':1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        # self.tokenizer.add_special_tokens(text)\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels_dict[label])\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model,train_loader, val_loader):\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        # self.test_loader = test_loader\n",
    "        self.device = config.DEVICE\n",
    "        self.learning_rate = config.LEARNING_RATE\n",
    "\n",
    "        # Example initial learning rate\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            betas=(config.B1, config.B2),\n",
    "            weight_decay=config._LAMBDA\n",
    "        )\n",
    "        self.scheduler = lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda epoch: 1 - epoch/(config.NUM_EPOCHS))\n",
    "        self.weight = self.calculate_class_weights().to(config.DEVICE)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=self.weight)\n",
    "        \n",
    "    def calculate_class_weights(self,csv_file='preprocess_data/arman_emo_concat.csv'):\n",
    "        labels_dict = {'SAD': 0, 'HAPPY': 1,'SURPRISE': 2, 'FEAR': 3, 'HATE': 4, 'ANGRY': 5,'OTHER': 6,}\n",
    "        data = pd.read_csv(csv_file)\n",
    "        labels = data['major_emotion']\n",
    "        class_counts = labels.value_counts()\n",
    "        total_samples = len(labels)\n",
    "        class_weights = {}\n",
    "        for label, count in class_counts.items():\n",
    "            class_weights[label] = total_samples / (len(class_counts) * count)\n",
    "        matched_class_weights = {label: class_weights[label] for label in labels_dict}\n",
    "        class_weights_tensor = torch.tensor(list(matched_class_weights.values()))\n",
    "        return class_weights_tensor\n",
    "\n",
    "\n",
    "    def train(self, num_epochs=config.NUM_EPOCHS):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc, train_f1 = self._train_epoch()\n",
    "            val_loss, val_acc, val_f1 = self._evaluate(self.val_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f} | Train F1 Score: {train_f1:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f} | Validation F1 Score: {val_f1:.4f}\")\n",
    "            print()\n",
    "\n",
    "            with open(config.LOGFILE, 'a') as f:\n",
    "                f.write(f\"Epoch {epoch+1}/{num_epochs} | \")\n",
    "                f.write(f\"Train Loss: {train_loss:.4f} | \")\n",
    "                f.write(f\"Train Accuracy: {train_acc:.4f} | \")\n",
    "                f.write(f\"Train F1 Score: {train_f1:.4f} | \")\n",
    "                f.write(f\"Valid Loss: {val_loss:.4f} | \")\n",
    "                f.write(f\"Valid Accuracy: {val_acc:.4f} | \")\n",
    "                f.write(f\"Valid F1 Score: {val_f1:.4f} | \")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "            checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'train_acc': train_acc,\n",
    "                        'train_f1_score': train_f1,\n",
    "                        'valid_loss': val_loss,\n",
    "                        'valid_acc': val_acc,\n",
    "                        'valid_f1_score': val_f1\n",
    "                    }\n",
    "            torch.save(checkpoint, f'checkpoints/{epoch+1}checkpoint.pth')\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_predictions = []\n",
    "        total_labels = []\n",
    "\n",
    "        for batch in tqdm(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(input_ids, attention_mask).logits\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            total_predictions.extend(predicted.cpu().tolist())\n",
    "            total_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        average_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = total_correct / total_samples\n",
    "        f1 = f1_score(total_labels, total_predictions, average='macro')\n",
    "        return average_loss, accuracy, f1\n",
    "\n",
    "    def _evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_predictions = []\n",
    "        total_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask).logits\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "                total_predictions.extend(predicted.cpu().tolist())\n",
    "                total_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        accuracy = total_correct / total_samples\n",
    "        f1 = f1_score(total_labels, total_predictions, average='macro')\n",
    "\n",
    "        \n",
    "        return average_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifierFactory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model(model_name, num_labels):\n",
    "        model_name = model_name.lower()\n",
    "        if model_name == 'parsbert':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\", num_labels=num_labels)\n",
    "        elif model_name == 'xlm-roberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained('checkpoints/models--xlm-roberta-large/snapshots/')\n",
    "            model = AutoModelForSequenceClassification.from_pretrained('checkpoints/models--xlm-roberta-large/snapshots/', num_labels=num_labels)\n",
    "        elif model_name == 'xlm-emo':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"m3hrdadfi/xlm-mlm-17-1280-emotion\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"m3hrdadfi/xlm-mlm-17-1280-emotion\", num_labels=num_labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' not supported.\")\n",
    "        \n",
    "        return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig , AutoTokenizer , AutoModel\n",
    "\n",
    "def change_grad(module,_req_grad_=False):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = _req_grad_\n",
    "    \n",
    "num_labels = 7\n",
    "model_name = 'parsbert'\n",
    "tokenizer , model = SequenceClassifierFactory().create_model(model_name,num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train,y_train,max_length=64)\n",
    "test_dataset = TextDataset(X_test,y_test,max_length=64)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:05<00:00,  2.07it/s]\n",
      "100%|██████████| 9/9 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.7964 | Train Accuracy: 0.2333 | Train F1 Score: 0.2313\n",
      "Validation Loss: 1.3261 | Validation Accuracy: 0.4926 | Validation F1 Score: 0.4859\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:21<00:00,  2.01it/s]\n",
      "100%|██████████| 9/9 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 1.5726 | Train Accuracy: 0.3175 | Train F1 Score: 0.3312\n",
      "Validation Loss: 1.1705 | Validation Accuracy: 0.5578 | Validation F1 Score: 0.5305\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:20<00:00,  2.01it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 1.3178 | Train Accuracy: 0.4167 | Train F1 Score: 0.4495\n",
      "Validation Loss: 1.4311 | Validation Accuracy: 0.5613 | Validation F1 Score: 0.5277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:21<00:00,  2.01it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.9954 | Train Accuracy: 0.5419 | Train F1 Score: 0.5857\n",
      "Validation Loss: 1.7872 | Validation Accuracy: 0.5387 | Validation F1 Score: 0.5219\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:01<00:00,  2.08it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.6518 | Train Accuracy: 0.6916 | Train F1 Score: 0.7318\n",
      "Validation Loss: 2.1360 | Validation Accuracy: 0.5091 | Validation F1 Score: 0.4833\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:00<00:00,  2.09it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.4080 | Train Accuracy: 0.8006 | Train F1 Score: 0.8310\n",
      "Validation Loss: 2.5437 | Validation Accuracy: 0.4778 | Validation F1 Score: 0.4476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:00<00:00,  2.08it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.2480 | Train Accuracy: 0.8760 | Train F1 Score: 0.8968\n",
      "Validation Loss: 3.0266 | Validation Accuracy: 0.4796 | Validation F1 Score: 0.4503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:00<00:00,  2.08it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.1762 | Train Accuracy: 0.9160 | Train F1 Score: 0.9288\n",
      "Validation Loss: 3.0550 | Validation Accuracy: 0.4970 | Validation F1 Score: 0.4777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:00<00:00,  2.09it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.1288 | Train Accuracy: 0.9392 | Train F1 Score: 0.9480\n",
      "Validation Loss: 3.3261 | Validation Accuracy: 0.4596 | Validation F1 Score: 0.4447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [09:00<00:00,  2.09it/s]\n",
      "100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.1005 | Train Accuracy: 0.9545 | Train F1 Score: 0.9614\n",
      "Validation Loss: 3.4920 | Validation Accuracy: 0.4396 | Validation F1 Score: 0.4236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(model=model,train_loader=train_dataloader,val_loader=test_dataloader)\n",
    "trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1418.10it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 84.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(texts,local_ids, model, device, threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    inputs = []  # To store the input tensors\n",
    "    # anger, sadness, surprise, happiness, fear, disgust, other\n",
    "    label_dict = {0: 'sadness', 1: 'happiness', 2: 'surprise', 3: 'fear', 4: 'disgust', 5: 'anger', 6: 'other'}\n",
    "    num_classes = len(label_dict)\n",
    "    # texts = texts['tweet'].tolist()\n",
    "    # local_id = texts['local_id'].tolist()\n",
    "    \n",
    "    # Tokenize and preprocess the texts\n",
    "    tokenizer = AutoTokenizer.from_pretrained('HooshvareLab/bert-base-parsbert-uncased')\n",
    "    max_length = 64\n",
    "    for text in tqdm(texts):\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze().to(device)\n",
    "        attention_mask = encoding['attention_mask'].squeeze().to(device)\n",
    "        inputs.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        prob_matrix = []  # To store the probability distribution for each text\n",
    "        for input_data in tqdm(inputs):\n",
    "            input_ids = input_data['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = input_data['attention_mask'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            outputs = model(input_ids, attention_mask).logits\n",
    "            predicted_probs = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
    "            prob_matrix.append(predicted_probs)\n",
    "            \n",
    "            # Apply threshold to determine emotions present in text\n",
    "            predicted_labels = [label_dict[idx] for idx, prob in enumerate(predicted_probs) if prob > threshold]\n",
    "            predictions.append(label_dict[torch.argmax(outputs, dim=1).item()])\n",
    "\n",
    "    csv_data = []\n",
    "    for local_id, text, primary_emotion, probs in zip(local_ids, texts, predictions, prob_matrix):\n",
    "        emotion_probs = [1 if prob > threshold else 0 for prob in probs]\n",
    "        row = [local_id, text, primary_emotion] + emotion_probs\n",
    "        csv_data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the CSV data\n",
    "    columns = [\"local_id\", \"tweet\", \"primary_emotion\", \"anger\", \"sadness\", \"fear\", \"happiness\", \"disgust\", \"surprise\", \"other\"]\n",
    "    df = pd.DataFrame(csv_data, columns=columns)\n",
    "    df.to_csv('checkpoints/predictions_2.csv', index=False)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "texts = pd.read_csv('final/data_emotion_without_label.csv',encoding='utf-8')['tweet'].tolist()\n",
    "local_id = pd.read_csv('final/data_emotion_without_label.csv',encoding='utf-8')['local_id'].tolist()\n",
    "\n",
    "x = predict(texts=texts,local_ids=local_id,model=model,device='cuda',threshold=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom imblearn.over_sampling import SMOTE\\nimport nlpaug.augmenter.word as naw\\n\\n# Load a text dataset (e.g., 20 Newsgroups)\\nnewsgroups = fetch_20newsgroups(subset=\\'all\\', remove=(\\'headers\\', \\'footers\\', \\'quotes\\'))\\n\\nX, y = newsgroups.data, newsgroups.target\\n\\n# Split the dataset into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Apply TF-IDF vectorization\\nvectorizer = TfidfVectorizer(max_features=10000)\\nX_train_tfidf = vectorizer.fit_transform(X_train)\\n\\n# Apply oversampling using SMOTE\\noversampler = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\\n\\n# Apply data augmentation using nlpaug\\naug = naw.SynonymAug(aug_src=\\'wordnet\\', aug_max=10)\\nX_train_augmented = [aug.augment(text) for text in X_train]\\n\\n# Print sample sizes before and after resampling/augmentation\\nprint(\"Original class distribution:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\\nprint(\"After SMOTE resampling:\", {class_label: sum(y_train_resampled == class_label) for class_label in set(y_train_resampled)})\\nprint(\"After data augmentation:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\\n\\n# Now you can use X_train_resampled and y_train_resampled for training\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Oversampling and Undersampling: Oversampling and undersampling techniques can be used with text data, but there are some challenges. In oversampling, simply duplicating text samples might not be effective, as it can lead to overfitting. Techniques like SMOTE or generating synthetic examples through text augmentation (e.g., paraphrasing) can be more appropriate. For undersampling, randomly removing text samples may result in loss of valuable information. Undersampling methods that consider text properties, like Tomek links, might be more suitable.\n",
    "\n",
    "Synthetic Data Generation: Generating synthetic examples for text data can be more complex than in tabular data. Techniques like SMOTE might need adaptations to consider the sequential nature of text. Additionally, methods like Word2Vec or GPT-based language models can be used for generating semantically similar but contextually different text samples.\n",
    "\n",
    "Data Augmentation: Data augmentation methods for text involve introducing variations in text content while preserving meaning. Techniques like synonym replacement, word swapping, and paraphrasing can be used to create augmented versions of the minority class text samples.\n",
    "\n",
    "Class-Weighted Loss: Class-weighted loss can be applied to text data as well. It considers the importance of each class in the loss calculation during training.\n",
    "\n",
    "Imbalanced-learn Library: If you're using Python, the imbalanced-learn library offers resampling techniques tailored for imbalanced datasets. While some techniques might need adjustments for text data, the library can still provide a good starting point.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load a text dataset (e.g., 20 Newsgroups)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apply oversampling using SMOTE\n",
    "oversampler = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Apply data augmentation using nlpaug\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_max=10)\n",
    "X_train_augmented = [aug.augment(text) for text in X_train]\n",
    "\n",
    "# Print sample sizes before and after resampling/augmentation\n",
    "print(\"Original class distribution:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\n",
    "print(\"After SMOTE resampling:\", {class_label: sum(y_train_resampled == class_label) for class_label in set(y_train_resampled)})\n",
    "print(\"After data augmentation:\", {class_label: sum(y_train == class_label) for class_label in set(y_train)})\n",
    "\n",
    "# Now you can use X_train_resampled and y_train_resampled for training\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any\n",
    "import hazm\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from dataclasses import dataclass\n",
    "from hazm import Lemmatizer\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self,text):\n",
    "        # Step 1: Normalize\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_english_chars(text)\n",
    "        # text = self.remove_repeatable_chars(text)\n",
    "        text = self.remove_arabic_diacritics(text)\n",
    "        text = self.remove_non_persian_chars(text)\n",
    "\n",
    "        text = self.remove_persian_numerals(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        return text\n",
    "    \n",
    "    # Step 1: Normalize\n",
    "    def normalize(self,text):\n",
    "        normalizer = Normalizer(\n",
    "            full_cleaning=True,\n",
    "        )\n",
    "        return normalizer.normalize(text)\n",
    "\n",
    "    # Step 2: remove any engish character\n",
    "    def remove_english_chars(self,text):\n",
    "        english_chars_pattern = re.compile(r'[a-zA-Z]')\n",
    "        cleaned_text = re.sub(english_chars_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 3: remove repeatable characters\n",
    "    def remove_repeatable_chars(self,text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    # Step 4: remove arabic diactrics\n",
    "    def remove_arabic_diacritics(self,text):\n",
    "        \"\"\"\n",
    "            Some common Arabic diacritical marks include:\n",
    "                Fatha (ً): Represents the short vowel \"a\" or \"u\" when placed above a letter.\n",
    "                Kasra (ٍ): Represents the short vowel \"i\" when placed below a letter.\n",
    "                Damma (ٌ): Represents the short vowel \"u\" when placed above a letter.\n",
    "                Sukun (ـْ): Indicates the absence of any vowel sound.\n",
    "                Shadda (ّ): Represents consonant doubling or gemination.\n",
    "                Tanween (ًٌٍ): Represents the nunation or the \"n\" sound at the end of a word.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "            The regular expression [\\u064B-\\u065F] represents a character range that covers the Unicode code points for Arabic diacritics.\n",
    "        \"\"\"\n",
    "        # مرحبا بكم <== \"مَرْحَبًا بِكُمْ\"\n",
    "        arabic_diacritics_pattern = re.compile(r'[\\u064B-\\u065F]')\n",
    "        cleaned_text = re.sub(arabic_diacritics_pattern, '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 5: remove any non-persian chars\n",
    "    def remove_non_persian_chars(self,text):\n",
    "        persian_chars_pattern = re.compile(r'[^\\u0600-\\u06FF\\uFB8A\\u067E\\u0686\\u06AF\\u200C\\u200F]+')\n",
    "        cleaned_text = re.sub(persian_chars_pattern, ' ', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Step 6: remove # sign from text while keeping the information included into hashtags\n",
    "    def remove_hashtags(self,text):\n",
    "        # Regular expression to match hashtags\n",
    "        hashtag_pattern = r'#\\w+'\n",
    "        \n",
    "        def extract_and_replace(match):\n",
    "            # Extract the text from the matched hashtag and remove the '#' sign\n",
    "            hashtag_text = match.group(0)[1:]\n",
    "            return hashtag_text\n",
    "        \n",
    "        # Use the 're.sub' function with the 'extract_and_replace' function as the replacement\n",
    "        cleaned_text = re.sub(hashtag_pattern, extract_and_replace, text)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "\n",
    "    # Step 7: remove persian numeric characters from text\n",
    "    def remove_persian_numerals(self,text):\n",
    "        # Define a translation table to map Persian numerals to None (remove them)\n",
    "        persian_numerals = {\n",
    "            ord('۰'): None,\n",
    "            ord('۱'): None,\n",
    "            ord('۲'): None,\n",
    "            ord('۳'): None,\n",
    "            ord('۴'): None,\n",
    "            ord('۵'): None,\n",
    "            ord('۶'): None,\n",
    "            ord('۷'): None,\n",
    "            ord('۸'): None,\n",
    "            ord('۹'): None\n",
    "        }\n",
    "        # Use str.translate() to remove Persian numerals\n",
    "        cleaned_text = text.translate(persian_numerals)\n",
    "        return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_emoPars.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17304\\2476935492.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_emoPars.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_emoPars.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_emoPars.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "train_file = pd.read_csv('train_emoPars.csv',encoding='utf-8')\n",
    "test_file = pd.read_csv('test_emoPars.csv',encoding='utf-8')\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "train_file['text'] = train_file['text'].progress_apply(p)\n",
    "test_file['text'] = test_file['text'].progress_apply(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.to_csv('train_cleaned_emopars.csv')\n",
    "test_file.to_csv('test_cleaned_emopars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.to_csv('test_cleaned_emopars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       من خیلی خودسانسوری می‌کنم تو اینستا هر چی فالو...\n",
       "1               بعد اتمام جلسه مجلس روند بازار برگشت بورس\n",
       "2       کاربران توییتر در جریان طوفان توییتری اعتراض ب...\n",
       "3       وحشی شدن معده بعد از رسیدن به ایران اجتناب ناپ...\n",
       "4       سحام نیوز بیانیه مشترک عربستان و امارات با پرو...\n",
       "                              ...                        \n",
       "2995                    بعد از کرونا یه صفایی بکنیم با هم\n",
       "2996                              قبلش یه پیتزا بده حداقل\n",
       "2997    از رفقا می‌خواهم که از بین اپوزیسیون حزب یا سا...\n",
       "2998    از هر دریچه‌ای؛ چه موازنه قوا با امریکا چه تام...\n",
       "2999          چند نفر تا حتما هستن میخوام آمار بگیرم بگین\n",
       "Name: texts, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 219.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "y = pd.read_csv('datasets/data_emotion_without_label.csv')\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "y['tweet'] = y['tweet'].progress_apply(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv('data_emotion_without_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7308/7308 [00:18<00:00, 386.83it/s]\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train,test],axis=0)\n",
    "p = Preprocessing()\n",
    "tqdm.pandas()\n",
    "combined['text'] = combined['text'].progress_apply(p)\n",
    "combined.to_csv('final/arman.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('checkpoints/predictions_1.csv')\n",
    "y = pd.read_csv('res.csv')\n",
    "y['primary_emotion'] = x['primary_emotion']\n",
    "y.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
