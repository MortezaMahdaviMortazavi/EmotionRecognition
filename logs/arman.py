# -*- coding: utf-8 -*-
"""arman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YbAXM32Su5ULIDXQ2wRU1ubzGCGuOaHL
"""

!pip install transformers
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from transformers import AutoModel,AutoTokenizer,AutoModelForSequenceClassification

train = open('train.txt',encoding='utf-8').readlines()
test = open('test.txt',encoding='utf-8').readlines()
X_train = []
y_train = []
X_test = []
y_test = []

for item in train:
    _X,_y = item.split(",")
    X_train.append(_X.strip())
    y_train.append(_y.strip())

for item in test:
    _X,_y = item.split(",")
    X_test.append(_X.strip())
    y_test.append(_y.strip())

tokenizer_path = '/content/drive/MyDrive/HuggingFace/XLM-Roberta-Base'
model_path = '/content/drive/MyDrive/HuggingFace/XLM-Roberta-Base'
from google.colab import drive
drive.mount('/content/drive')

from transformers import DistilBertTokenizer, DistilBertModel,AutoModelForSequenceClassification
class XLMRobertaGRUClassifier(nn.Module):
    def __init__(self, num_classes):
        super(XLMRobertaGRUClassifier, self).__init__()
        self.num_classes = num_classes
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.xlmroberta = AutoModel.from_pretrained(model_path)
        self.gru = nn.GRU(self.xlmroberta.config.hidden_size, hidden_size=256, num_layers=1, batch_first=True,bidirectional=True)
        self.linear = nn.Linear(512, num_classes)
        self.dropout = nn.Dropout(0.2)

    def forward(self, input_ids, attention_mask):
        outputs = self.xlmroberta(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = outputs.last_hidden_state
        gru_output, _ = self.gru(embeddings)
        logits = self.linear(self.dropout(gru_output[:, -1, :]))
        return logits

# import shutil
# from google.colab import drive
# drive.mount('/content/drive')
# shutil.move(model, '/content/drive/MyDrive')

arman = pd.DataFrame({'tweet': X_train, 'emotion': y_train})

def calculate_class_weights(df, target_column, labels_dict):
    emotions = df[target_column]
    class_indices = emotions.map(labels_dict)
    class_counts = class_indices.value_counts().sort_index()
    total_samples = len(class_indices)
    class_weights = torch.tensor(total_samples / (class_counts * len(class_counts)), dtype=torch.float)
    return class_weights
# arman = pd.DataFrame({'text': X_train, 'label': y_train})
target_column = 'emotion'
labels_dict = {'SAD': 0, 'HAPPY': 1,'SURPRISE': 2, 'FEAR': 3, 'HATE': 4, 'ANGRY': 5,'OTHER': 6}
class_weights = calculate_class_weights(arman, target_column, labels_dict)
class_weights

# X_train = train['text'].tolist()
# y_train = train['label'].tolist()
# X_test = test['text'].tolist()
# y_test = test['label'].tolist()

# combined['emotion'].value_counts()

# from sklearn.model_selection import train_test_split
# X , y = dataset['tweet'].tolist() , dataset['emotion'].tolist()
# train , test = train_test_split(combined,test_size=0.1,stratify=combined['emotion'])
# X_train , y_train = train['tweet'].tolist() , train['emotion'].tolist()
# X_test , y_test = test['tweet'].tolist() , test['emotion'].tolist()

# import math
class TextDataset(torch.utils.data.Dataset):
    def __init__(self,tokenizer,texts, labels, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.labels_dict = {'SAD': 0, 'HAPPY': 1,'SURPRISE': 2, 'FEAR': 3, 'HATE': 4, 'ANGRY': 5,'OTHER': 6}
        # self.labels_dict = {'ANGRY':0,'HATE':1,'FEAR':2,'SAD':3,'HAPPY':4,'SURPRISE':5,'OTHER':6}

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        # self.tokenizer.add_special_tokens(text)
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt'
        )

        inputs = {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(self.labels_dict[label])
        }
        return inputs

# from google.colab import drive
# drive.mount('/content/drive')
# XLM_BASE_DIR = 'drive/MyDrive/HuggingFace/XLM-Roberta-Base'

tokenizer = AutoTokenizer.from_pretrained(model_path)
# model = AutoModelForSequenceClassification.from_pretrained(XLM_BASE_DIR, num_labels=7)

num_epochs = 10
train_dataset = TextDataset(tokenizer,X_train,y_train,max_length=100)
test_dataset = TextDataset(tokenizer,X_test,y_test,max_length=100)
train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=64,shuffle=False)
model = XLMRobertaGRUClassifier(num_classes=7)

torch.cuda.is_available()

from sklearn.metrics import accuracy_score, f1_score
from torch.optim.lr_scheduler import StepLR
from tqdm import tqdm
import math
from transformers import get_linear_schedule_with_warmup

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cuda')
criterion = nn.CrossEntropyLoss()
model = model.to(device)
# model.load_state_dict(torch.load('XLM_GRU_ROBERTA_PE.pt'))
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)
total_training_steps = num_epochs * 80
warmup_proportion = 0.1
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=math.ceil(total_training_steps * warmup_proportion),
                                            num_training_steps=total_training_steps)

# dicts = torch.load('/content/drive/MyDrive/XLMRobertaOnArman.pt')
# model.load_state_dict(dicts)

for _iter in range(num_epochs):
    train_loss = 0.0
    train_predictions = []
    train_labels = []
    model.train()
    for idx,batch in enumerate(tqdm(train_dataloader)):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids,attention_mask)
        loss = criterion(outputs,labels)
        # Backward pass and update model parameters
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        scheduler.step()

        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()
        train_predictions.extend(batch_predictions)
        train_labels.extend(labels.cpu().tolist())
        torch.save(model.state_dict(),'XLM_GRU_ROBERTA_PE_weights.pt')
        torch.save(model,'XLM_GRU_ROBERTA_PE_graph.pt')

        train_loss+=loss.item()
        if idx % 10 == 0:
            print("Epoch:",_iter,"batch:",idx,"loss:",loss.item())

        # Calculate accuracy and F-score
    train_accuracy = accuracy_score(train_labels, train_predictions)
    train_fscore = f1_score(train_labels, train_predictions, average='macro')

    print("Total Loss:", train_loss / len(train_dataloader))
    print("Train Accuracy:", train_accuracy)
    print("Train F-score:", train_fscore)


    test_predictions = []
    test_labels = []
    test_loss = 0.0
    model.eval()
    for idx, batch in enumerate(test_dataloader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)

        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()
        test_predictions.extend(batch_predictions)
        test_labels.extend(labels.cpu().tolist())
        test_loss += loss.item()
    test_accuracy = accuracy_score(test_labels, test_predictions)
    test_fscore = f1_score(test_labels, test_predictions, average='macro')
    print("Total Loss (Test):", test_loss / len(test_dataloader))
    print("Test Accuracy:", test_accuracy)
    print("Test F-score:", test_fscore)
    # break

    # torch.save(model.state_dict(),'checkpoints/ParsBertOnTweets100k.pt')

torch.cuda.is_available()

from tqdm import tqdm
def predict(texts,local_ids, model, device, threshold=0.5):
    model.eval()  # Set the model to evaluation mode
    inputs = []  # To store the input tensors
    # anger, sadness, surprise, happiness, fear, disgust, other
    label_dict = {0: 'sadness', 1: 'happiness', 2: 'surprise', 3: 'fear', 4: 'disgust', 5: 'anger', 6: 'other'}
    num_classes = len(label_dict)
    # texts = texts['tweet'].tolist()
    # local_id = texts['local_id'].tolist()

    # Tokenize and preprocess the texts
    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
    max_length = 64
    for text in tqdm(texts):
        encoding = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            truncation=True,
            max_length=max_length,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze().to(device)
        attention_mask = encoding['attention_mask'].squeeze().to(device)
        inputs.append({'input_ids': input_ids, 'attention_mask': attention_mask})

    # Make predictions
    with torch.no_grad():
        predictions = []
        prob_matrix = []  # To store the probability distribution for each text
        for input_data in tqdm(inputs):
            input_ids = input_data['input_ids'].unsqueeze(0)  # Add batch dimension
            attention_mask = input_data['attention_mask'].unsqueeze(0)  # Add batch dimension

            outputs = model(input_ids, attention_mask).logits
            predicted_probs = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()
            prob_matrix.append(predicted_probs)

            # Apply threshold to determine emotions present in text
            predicted_labels = [label_dict[idx] for idx, prob in enumerate(predicted_probs) if prob > threshold]
            predictions.append(label_dict[torch.argmax(outputs, dim=1).item()])

    csv_data = []
    for local_id, text, primary_emotion, probs in zip(local_ids, texts, predictions, prob_matrix):
        emotion_probs = [1 if prob > threshold else 0 for prob in probs]
        row = [local_id, text, primary_emotion] + emotion_probs
        csv_data.append(row)

    # Create a DataFrame from the CSV data
    columns = ["local_id", "tweet", "primary_emotion", "anger", "sadness", "fear", "happiness", "disgust", "surprise", "other"]
    df = pd.DataFrame(csv_data, columns=columns)
    df.to_csv('predictions_2.csv', index=False)

    return predictions


texts = pd.read_csv('data_emotion_without_label.csv',encoding='utf-8')['tweet'].tolist()
local_id = pd.read_csv('data_emotion_without_label.csv',encoding='utf-8')['local_id'].tolist()

x = predict(texts=texts,local_ids=local_id,model=model,device='cuda',threshold=0.35)

import shutil
shutil.move('DistillBert.pt', '/content/drive/MyDrive')

"""
Oversampling and Undersampling: Oversampling and undersampling techniques can be used with text data, but there are some challenges. In oversampling, simply duplicating text samples might not be effective, as it can lead to overfitting. Techniques like SMOTE or generating synthetic examples through text augmentation (e.g., paraphrasing) can be more appropriate. For undersampling, randomly removing text samples may result in loss of valuable information. Undersampling methods that consider text properties, like Tomek links, might be more suitable.

Synthetic Data Generation: Generating synthetic examples for text data can be more complex than in tabular data. Techniques like SMOTE might need adaptations to consider the sequential nature of text. Additionally, methods like Word2Vec or GPT-based language models can be used for generating semantically similar but contextually different text samples.

Data Augmentation: Data augmentation methods for text involve introducing variations in text content while preserving meaning. Techniques like synonym replacement, word swapping, and paraphrasing can be used to create augmented versions of the minority class text samples.

Class-Weighted Loss: Class-weighted loss can be applied to text data as well. It considers the importance of each class in the loss calculation during training.

Imbalanced-learn Library: If you're using Python, the imbalanced-learn library offers resampling techniques tailored for imbalanced datasets. While some techniques might need adjustments for text data, the library can still provide a good starting point.

"""

"""
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
import nlpaug.augmenter.word as naw

# Load a text dataset (e.g., 20 Newsgroups)
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

X, y = newsgroups.data, newsgroups.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=10000)
X_train_tfidf = vectorizer.fit_transform(X_train)

# Apply oversampling using SMOTE
oversampler = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)

# Apply data augmentation using nlpaug
aug = naw.SynonymAug(aug_src='wordnet', aug_max=10)
X_train_augmented = [aug.augment(text) for text in X_train]

# Print sample sizes before and after resampling/augmentation
print("Original class distribution:", {class_label: sum(y_train == class_label) for class_label in set(y_train)})
print("After SMOTE resampling:", {class_label: sum(y_train_resampled == class_label) for class_label in set(y_train_resampled)})
print("After data augmentation:", {class_label: sum(y_train == class_label) for class_label in set(y_train)})

# Now you can use X_train_resampled and y_train_resampled for training


"""

import re
from typing import Any
import hazm
from dadmatools.models.normalizer import Normalizer
from dataclasses import dataclass
from hazm import Lemmatizer
class Preprocessing:
    def __init__(self):
        pass

    def __call__(self,text):
        # Step 1: Normalize
        text = self.normalize(text)
        text = self.remove_english_chars(text)
        # text = self.remove_repeatable_chars(text)
        text = self.remove_arabic_diacritics(text)
        text = self.remove_non_persian_chars(text)

        text = self.remove_persian_numerals(text)
        text = self.remove_hashtags(text)
        return text

    # Step 1: Normalize
    def normalize(self,text):
        normalizer = Normalizer(
            full_cleaning=True,
        )
        return normalizer.normalize(text)

    # Step 2: remove any engish character
    def remove_english_chars(self,text):
        english_chars_pattern = re.compile(r'[a-zA-Z]')
        cleaned_text = re.sub(english_chars_pattern, '', text)
        return cleaned_text

    # Step 3: remove repeatable characters
    def remove_repeatable_chars(self,text):
        return hazm.Normalizer().normalize(text)

    # Step 4: remove arabic diactrics
    def remove_arabic_diacritics(self,text):
        """
            Some common Arabic diacritical marks include:
                Fatha (ً): Represents the short vowel "a" or "u" when placed above a letter.
                Kasra (ٍ): Represents the short vowel "i" when placed below a letter.
                Damma (ٌ): Represents the short vowel "u" when placed above a letter.
                Sukun (ـْ): Indicates the absence of any vowel sound.
                Shadda (ّ): Represents consonant doubling or gemination.
                Tanween (ًٌٍ): Represents the nunation or the "n" sound at the end of a word.
        """

        """
            The regular expression [\u064B-\u065F] represents a character range that covers the Unicode code points for Arabic diacritics.
        """
        # مرحبا بكم <== "مَرْحَبًا بِكُمْ"
        arabic_diacritics_pattern = re.compile(r'[\u064B-\u065F]')
        cleaned_text = re.sub(arabic_diacritics_pattern, '', text)
        return cleaned_text

    # Step 5: remove any non-persian chars
    def remove_non_persian_chars(self,text):
        persian_chars_pattern = re.compile(r'[^\u0600-\u06FF\uFB8A\u067E\u0686\u06AF\u200C\u200F]+')
        cleaned_text = re.sub(persian_chars_pattern, ' ', text)
        return cleaned_text

    # Step 6: remove # sign from text while keeping the information included into hashtags
    def remove_hashtags(self,text):
        # Regular expression to match hashtags
        hashtag_pattern = r'#\w+'

        def extract_and_replace(match):
            # Extract the text from the matched hashtag and remove the '#' sign
            hashtag_text = match.group(0)[1:]
            return hashtag_text

        # Use the 're.sub' function with the 'extract_and_replace' function as the replacement
        cleaned_text = re.sub(hashtag_pattern, extract_and_replace, text)

        return cleaned_text


    # Step 7: remove persian numeric characters from text
    def remove_persian_numerals(self,text):
        # Define a translation table to map Persian numerals to None (remove them)
        persian_numerals = {
            ord('۰'): None,
            ord('۱'): None,
            ord('۲'): None,
            ord('۳'): None,
            ord('۴'): None,
            ord('۵'): None,
            ord('۶'): None,
            ord('۷'): None,
            ord('۸'): None,
            ord('۹'): None
        }
        # Use str.translate() to remove Persian numerals
        cleaned_text = text.translate(persian_numerals)
        return cleaned_text

import pandas as pd
from tqdm import tqdm
train_file = pd.read_csv('train_emoPars.csv',encoding='utf-8')
test_file = pd.read_csv('test_emoPars.csv',encoding='utf-8')
p = Preprocessing()
tqdm.pandas()
train_file['text'] = train_file['text'].progress_apply(p)
test_file['text'] = test_file['text'].progress_apply(p)

train_file.to_csv('train_cleaned_emopars.csv')
test_file.to_csv('test_cleaned_emopars.csv')

train_file.to_csv('test_cleaned_emopars.csv')

train_file['texts']

import pandas as pd
from tqdm import tqdm
y = pd.read_csv('datasets/data_emotion_without_label.csv')
p = Preprocessing()
tqdm.pandas()
y['tweet'] = y['tweet'].progress_apply(p)

y.to_csv('data_emotion_without_label.csv')

train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

combined = pd.concat([train,test],axis=0)
p = Preprocessing()
tqdm.pandas()
combined['text'] = combined['text'].progress_apply(p)
combined.to_csv('final/arman.csv')

x = pd.read_csv('checkpoints/predictions_1.csv')
y = pd.read_csv('res.csv')
y['primary_emotion'] = x['primary_emotion']
y.to_csv('final.csv')

anger = pd.read_csv('PersianTweets/anger.csv')
disgust = pd.read_csv('PersianTweets/disgust.csv')
fear = pd.read_csv('PersianTweets/fear.csv')
joy = pd.read_csv('PersianTweets/joy.csv')
sad = pd.read_csv('PersianTweets/sad.csv')
surprise = pd.read_csv('PersianTweets/surprise.csv')

len(anger) , len(disgust) , len(fear) , len(joy) , len(sad) , len(surprise)

all_tweets = pd.concat([anger,disgust,fear,joy,sad,surprise],axis=0)
all_tweets.to_csv('PersianTweets/allDataset.csv')

pd.read_csv('PersianTweets/allDataset.csv')

